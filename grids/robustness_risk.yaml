---
# Robustness/Sensitivity Analysis: Risk Parameters
# =================================================
# Test how sensitive the strategy is to small parameter changes.
#
# Goal: "How fragile is this strategy?"
#
# Method:
# - Pick a baseline configuration (e.g., risk=1%, SL=5%, TP=10%)
# - Apply small perturbations: ±10%, ±20%, ±30%
# - Measure performance variance
#
# If small changes cause huge performance swings → Strategy is fragile/overfit
# If performance is stable → Strategy is robust

name: "robustness_risk_v1"
description: "Robustness analysis for risk management parameters"
experiment_type: "robustness"

# Use grid search to test all perturbations
search_method: "grid"

# Train/test split
split:
  train_start: "2018-01-01"
  train_end: "2022-12-31"
  test_start: "2023-01-01"
  test_end: "2025-01-17"

# Symbols
symbols: []

# =================================
# Baseline Configuration
# =================================
# Baseline (assumed optimal from previous grid search):
# - risk_per_trade: 1.0%
# - stop_loss: 5.0%
# - take_profit: 10.0%
#
# We'll test these with ±20% and ±30% perturbations

# =================================
# Robustness Grid
# =================================
parameters:
  # Risk per trade: 1.0% ± 20%, ± 30%
  # Values: 0.7, 0.8, 1.0, 1.2, 1.3
  risk_management.position_sizing.risk_per_trade_pct:
    values: [0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3]
    description: "Risk per trade - baseline 1.0% with perturbations"

  # Stop-loss: 5.0% ± 20%, ± 30%
  # Values: 3.5, 4.0, 5.0, 6.0, 6.5
  risk_management.stop_loss.percentage:
    values: [3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5]
    description: "Stop-loss % - baseline 5.0% with perturbations"

  # Take-profit: 10.0% ± 20%, ± 30%
  # Values: 7.0, 8.0, 10.0, 12.0, 13.0
  risk_management.take_profit.percentage:
    values: [7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0]
    description: "Take-profit % - baseline 10.0% with perturbations"

# Optional: Test analyst weight perturbations
  # analist_1.weight:
  #   values: [0.8, 0.9, 1.0, 1.1, 1.2]
  #   description: "Technical analyst weight perturbations"

# =================================
# Optimization Objective
# =================================
optimization:
  primary_metric: "test_sharpe"
  direction: "maximize"

  secondary_metrics:
    - "test_cagr"
    - "test_max_dd"
    - "test_win_rate"
    - "test_profit_factor"

  overfitting_check:
    enabled: true
    train_test_ratio_threshold: 1.5

# =================================
# Constraints
# =================================
constraints:
  test_sharpe_min: 0.0           # No minimum (want to see degradation)
  test_max_dd_max: -100.0        # No constraint
  test_min_trades: 5

# =================================
# Execution Settings
# =================================
execution:
  parallel_workers: 1
  save_equity_curves: false
  save_trade_lists: true
  verbose: true

# =================================
# Robustness Metrics to Analyze
# =================================
# After running, calculate:
#
# 1. Performance variance:
#    - Std dev of test_sharpe across all configs
#    - High variance = fragile, low variance = robust
#
# 2. Sensitivity per parameter:
#    - Risk: max_sharpe - min_sharpe when varying risk only
#    - SL: max_sharpe - min_sharpe when varying SL only
#    - TP: max_sharpe - min_sharpe when varying TP only
#
# 3. Worst-case degradation:
#    - Compare baseline to worst perturbation
#    - If baseline Sharpe = 1.5, worst = 0.5 → 67% drop = very fragile
#
# 4. Performance heatmap:
#    - Plot Sharpe vs (risk, SL) or (SL, TP)
#    - Look for flat regions (robust) vs steep cliffs (fragile)
#
# Good robustness:
# - Sharpe variance < 0.3
# - ±20% param change → <20% performance change
# - Smooth performance surface (no cliffs)
#
# Bad robustness:
# - Sharpe variance > 0.5
# - Small param change → huge performance swing
# - Performance "cliff edges" (overfit to specific values)
